{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP/UPUaiymYorKKpLlk5XOd",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/tradingwithme/Works-w-ML-and-w-o-ML/blob/main/Spring2025_Reinforcement_Learning_with_Neuromodulated_Spiking_Networks.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q brian2"
      ],
      "metadata": {
        "id": "V1j9MXGmPNA-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from brian2 import *\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm import tqdm"
      ],
      "metadata": {
        "id": "81AcY-GLcG3P"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Reinforcement Learning with Neuromodulated Spiking Networks\n",
        "\n",
        "## Theoretical Foundations\n",
        "\n",
        "### 1. Reinforcement Learning (RL) Overview\n",
        "Reinforcement learning is a computational framework where an agent learns to maximize cumulative reward through environment interactions. Key components:\n",
        "\n",
        "\\begin{align*}\n",
        "&\\textbf{State } s_t \\in \\mathcal{S} \\\\\n",
        "&\\textbf{Action } a_t \\in \\mathcal{A} \\\\\n",
        "&\\textbf{Policy } \\pi(a_t|s_t) \\\\\n",
        "&\\textbf{Reward } r_{t+1}\n",
        "\\end{align*}\n",
        "\n",
        "In spiking neural networks, actions are selected based on neuronal spike patterns rather than Q-values."
      ],
      "metadata": {
        "id": "xa4KLOF-b1uC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "### 2. Spike-Timing-Dependent Plasticity (STDP)\n",
        "STDP is a biologically observed Hebbian learning rule where synaptic efficacy changes based on precise spike timing:\n",
        "\n",
        "\\begin{equation*}\n",
        "\\Delta w_{ij} =\n",
        "\\begin{cases}\n",
        "\\eta \\cdot e^{-\\Delta t/\\tau_+} & \\text{if } \\Delta t > 0 \\\\\n",
        "-\\eta \\cdot e^{\\Delta t/\\tau_-} & \\text{if } \\Delta t < 0\n",
        "\\end{cases}\n",
        "\\end{equation*}\n",
        "\n",
        "where $\\Delta t = t_{post} - t_{pre}$. In **reward-modulated STDP (R-STDP)**, updates are scaled by global reward signals."
      ],
      "metadata": {
        "id": "iigkEVdYb8Q-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### 3. Neuromodulation: ACh vs. GABA\n",
        "| **Feature**       | **Acetylcholine (ACh)**                          | **GABA**                              |\n",
        "|--------------------|------------------------------------------------|---------------------------------------|\n",
        "| **Role**          | Enhances plasticity and learning               | Primary inhibitory neurotransmitter  |\n",
        "| **Effect in Model**| Scales STDP learning rate ($\\eta \\rightarrow ACh \\cdot \\eta$) | Scales WTA inhibition strength |\n",
        "| **Biological Target**| Muscarinic/nicotinic receptors              | GABA$_A$/GABA$_B$ receptors         |"
      ],
      "metadata": {
        "id": "0jihPJsQcAMG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Action Selection\n",
        "Actions are chosen via winner-take-all (WTA) dynamics based on spike counts:\n",
        "\n",
        "```python\n",
        "spike_counts = np.bincount(spike_mon.i, minlength=n_actions)\n",
        "action = np.argmax(spike_counts)  # Policy π(a|s)\n"
      ],
      "metadata": {
        "id": "pM4jvHjWd8nE"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reward Modulation\n",
        "Global reward $R$ scales all STDP updates:\n",
        "\n",
        "```python\n",
        "def update_weights(self, reward):\n",
        "    self.synapses.R = reward  # R-STDP\n"
      ],
      "metadata": {
        "id": "sNcnGnamfJIy"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mathematical Formulation\n",
        "\n",
        "### STDP Weight Update with ACh Scaling\n",
        "$$\n",
        "w = \\text{clip} \\left( w + R \\cdot ACh_{\\text{level}} \\cdot (a_{\\text{post}} - a_{\\text{pre}}), 0, w_{\\max} \\right)\n",
        "$$\n",
        "\n",
        "### GABA-Mediated Inhibition\n",
        "$$\n",
        "v_{\\text{post}} -= 15mV \\cdot GABA_{\\text{level}}\n",
        "$$\n"
      ],
      "metadata": {
        "id": "ZB6k5PUjkz9I"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Experimental Results: Predicted Outcomes\n",
        "| **Condition** | **Learning Speed** | **Weight Distribution** | **Biological Interpretation** |\n",
        "|--------------|------------------|----------------------|--------------------------|\n",
        "| **Control**  | Baseline         | Moderate variance  | Balanced excitation/inhibition |\n",
        "| **High ACh** | Faster           | High variance      | Enhanced LTP/LTD |\n",
        "| **High GABA** | Slower          | Low variance       | Over-stabilization |"
      ],
      "metadata": {
        "id": "CCmUL2Y8euyf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Network Parameters\n",
        "\n",
        "## Neuron Model\n",
        "- **Membrane Time Constant**: $\\tau_m = 10ms$ (controls decay of membrane potential)\n",
        "- **Spike Threshold**: $V_{\\text{th}} = 10mV$ (spiking occurs when voltage exceeds this)\n",
        "- **Reset Potential**: $V_{\\text{reset}} = 0mV$ (voltage resets after a spike)\n",
        "- **Dynamics**:\n",
        "  $$\n",
        "  \\frac{dv}{dt} = -\\frac{v}{\\tau_m}\n",
        "  $$\n",
        "\n",
        "## STDP Parameters\n",
        "- **Pre-Synaptic Trace Decay**: $\\tau_{\\text{pre}} = 20ms$\n",
        "- **Post-Synaptic Trace Decay**: $\\tau_{\\text{post}} = 20ms$\n",
        "- **Maximum Synaptic Weight**: $w_{\\text{max}} = 2.0$\n",
        "\n",
        "## Neuromodulation Baseline Levels\n",
        "- **Acetylcholine (ACh) Scaling Factor**: $ACh_{\\text{baseline}} = 1.0$ (enhances learning rate)\n",
        "- **GABA Scaling Factor**: $GABA_{\\text{baseline}} = 1.0$ (modulates inhibition strength)"
      ],
      "metadata": {
        "id": "jvZjBY1immtv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 1: NETWORK PARAMETERS\n",
        "# Neuron model\n",
        "# Membrane time constant\n",
        "tau_m = 10*ms\n",
        "# Spike threshold\n",
        "V_th = 10*mV\n",
        "V_reset = 0*mV      # Reset potential\n",
        "eqs = '''\n",
        "dv/dt = -v/tau_m : volt (unless refractory)\n",
        "'''\n",
        "\n",
        "# Next is to define the STDP parameters: pre-synaptic time constant, post-synaptic time constant, maximum synaptic weight\n",
        "taupre = 20*ms\n",
        "taupost = 20*ms\n",
        "wmax = 2.0"
      ],
      "metadata": {
        "id": "1FrLAlLnRY1U"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Neuromodulatory Effects\n",
        "\n",
        "\\begin{array}{|c|c|c|}\n",
        "\\hline\n",
        "\\textbf{Feature} & \\textbf{Acetylcholine (ACh)} & \\textbf{GABA} \\\\\n",
        "\\hline\n",
        "\\text{Role} & \\text{Enhances plasticity and learning} & \\text{Primary inhibitory neurotransmitter} \\\\\n",
        "\\hline\n",
        "\\text{Effect in Model} & \\eta \\rightarrow ACh \\cdot \\eta & \\text{Scales WTA inhibition strength} \\\\\n",
        "\\hline\n",
        "\\text{Biological Target} & \\text{Muscarinic/nicotinic receptors} & \\text{GABA}_A/\\text{GABA}_B \\text{ receptors} \\\\\n",
        "\\hline\n",
        "\\end{array}\n",
        "\n",
        "| **Feature**        | **Acetylcholine (ACh)**                       | **GABA**                         |\n",
        "|--------------------|----------------------------------------------|----------------------------------|\n",
        "| **Effect**        | Directly scales STDP magnitude               | Scales lateral inhibition        |\n",
        "| **Implementation** | $w += ACh\\_level \\cdot (a_{\\text{post}} - a_{\\text{pre}})$ | $v_{\\text{post}} -= 15mV \\cdot GABA\\_level$ |\n",
        "\n",
        "\n",
        "```python\n",
        "# ACh: Directly scales STDP magnitude\n",
        "w += ACh_level * (apost - apre)  # in on_pre rule\n",
        "\n",
        "# GABA: Scales lateral inhibition\n",
        "v_post -= 15*mV * GABA_level  # Stronger inhibition\n"
      ],
      "metadata": {
        "id": "keUVFFZCn0QM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Neuromodulation baseline levels\n",
        "ACh_baseline = 1.0  # Acetylcholine scaling factor\n",
        "GABA_baseline = 1.0 # GABA scaling factor"
      ],
      "metadata": {
        "id": "xjklocftnKNU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 2: Network Construction\n",
        "\n",
        "## Overview\n",
        "This spiking neural network incorporates **neuromodulation**:\n",
        "- **Acetylcholine (ACh)** modulates the STDP learning rate.\n",
        "- **GABA** scales inhibitory connections to regulate competition.\n",
        "\n",
        "## Network Components\n",
        "1. **Input Layer:** Poisson neurons representing different states.\n",
        "2. **Output Layer:** Leaky integrate-and-fire (LIF) neurons for action selection.\n",
        "3. **Inhibitory Connections:**\n",
        "   - **GABAergic competition** ensures winner-takes-all (WTA) dynamics.\n",
        "   - Synaptic weight scaled by **GABA level**:  \n",
        "     $$\n",
        "     v_{\\text{post}} -= w_{\\text{inh}} \\cdot mV\n",
        "     $$\n",
        "\n",
        "## Synaptic Model: Reward-Modulated STDP\n",
        "Each synapse maintains plasticity parameters:\n",
        "- **Synaptic Weight**: $w$\n",
        "- **Pre/Post Traces**: $a_{\\text{pre}}, a_{\\text{post}}$\n",
        "- **Reward Signal**: $R$\n",
        "- **ACh Influence**: Scales STDP update.\n",
        "\n",
        "### STDP Weight Update with ACh Scaling\n",
        "$$\n",
        "w = \\text{clip} \\left( w + R \\cdot ACh_{\\text{level}} \\cdot (a_{\\text{post}} - a_{\\text{pre}}), 0, w_{\\max} \\right)\n",
        "$$\n",
        "\n",
        "## Implementation in Code\n",
        "```python\n",
        "# ACh: Enhances STDP-driven learning\n",
        "w += ACh_level * (apost - apre)  # in on_pre rule\n",
        "\n",
        "# GABA: Strengthens lateral inhibition for WTA competition\n",
        "v_post -= 15*mV * GABA_level  # Scales inhibition strength\n"
      ],
      "metadata": {
        "id": "1SZd36w_sGTB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 2: NETWORK CONSTRUCTION\n",
        "# Builds a spiking network with neuromodulation:\n",
        "#  - ACh modulates STDP learning rate\n",
        "#  - GABA scales inhibitory connections\n",
        "def build_network(n_states=16, n_actions=4, ACh_level=1.0, GABA_level=1.0):\n",
        "    input_group = PoissonGroup(n_states, rates=0*Hz)\n",
        "    output_group = NeuronGroup(n_actions, eqs, threshold='v>V_th',\n",
        "                             reset='v=V_reset', refractory=2*ms, method='euler')\n",
        "    inh_connections = Synapses(output_group, output_group,\n",
        "                             model='''w_inh : 1''',\n",
        "                             on_pre='v_post -= w_inh*mV')\n",
        "    inh_connections.connect(condition='i != j')\n",
        "    inh_connections.w_inh = 15.0 * GABA_level  # GABA scales inhibition\n",
        "\n",
        "    # Cholinergic modulation of STDP\n",
        "    # Define apre and apost as state variables\n",
        "    syn_model = '''\n",
        "    w : 1              # Synaptic weight\n",
        "    apre : 1           # STDP pre-synaptic trace\n",
        "    apost : 1           # STDP post-synaptic trace\n",
        "    Apre : 1 (constant) # STDP pre-synaptic trace increment\n",
        "    Apost : 1 (constant) # STDP post-synaptic trace increment\n",
        "    R : 1              # Reward signal\n",
        "    ACh_level : 1 (constant) # Acetylcholine scaling factor # Added line\n",
        "    '''\n",
        "\n",
        "    on_pre = '''\n",
        "    v_post += w*mV\n",
        "    apre += Apre\n",
        "    w = clip(w + (R * ACh_level * (apost - apre)), 0, wmax)  # ACh scales STDP\n",
        "    '''\n",
        "    on_post = '''\n",
        "    apost += Apost\n",
        "    '''\n",
        "\n",
        "    synapses = Synapses(input_group, output_group, model=syn_model,\n",
        "                       on_pre=on_pre, on_post=on_post, method='euler')\n",
        "    synapses.connect()\n",
        "    synapses.w = 'rand()'  # Initialize random weights\n",
        "    synapses.Apre = 0.01\n",
        "    synapses.Apost = 0.01\n",
        "    synapses.ACh_level = ACh_level\n",
        "\n",
        "    # Monitors\n",
        "    spike_mon = SpikeMonitor(output_group)\n",
        "    state_mon = StateMonitor(synapses, 'w', record=True)\n",
        "\n",
        "    return input_group, output_group, synapses, inh_connections, spike_mon, state_mon"
      ],
      "metadata": {
        "id": "2Jda2o4gnXlU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 3: Reinforcement Learning Agent (RSTDPAgent)\n",
        "\n",
        "## Overview\n",
        "The `RSTDPAgent` implements **reinforcement learning with reward-modulated STDP** in a spiking neural network. It:\n",
        "- **Encodes state information** using Poisson neurons.\n",
        "- **Selects actions** based on spike activity in the output layer.\n",
        "- **Modulates synaptic plasticity** using a reward signal.\n",
        "\n",
        "---\n",
        "\n",
        "## Network Initialization\n",
        "Each agent builds its own spiking network with neuromodulation:\n",
        "- **ACh (Acetylcholine)** scales STDP learning rate.\n",
        "- **GABA** controls lateral inhibition in action selection."
      ],
      "metadata": {
        "id": "HDDpbt_8sZEK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 3: RL AGENT CLASS\n",
        "class RSTDPAgent:\n",
        "    def __init__(self, n_states, n_actions, ACh_level=1.0, GABA_level=1.0):\n",
        "        self.n_states = n_states\n",
        "        self.n_actions = n_actions\n",
        "        self.ACh_level = ACh_level\n",
        "        self.GABA_level = GABA_level\n",
        "        (self.input_group, self.output_group,\n",
        "         self.synapses, self.inh_connections,\n",
        "         self.spike_mon, self.state_mon) = build_network(n_states, n_actions,\n",
        "                                                        ACh_level, GABA_level)\n",
        "\n",
        "        # One hot state encoding :D for initialization\n",
        "        self.state_encodings = np.eye(n_states)\n",
        "\n",
        "        self.net = Network(self.input_group, self.output_group, self.synapses,\n",
        "                     self.inh_connections, self.spike_mon)\n",
        "\n",
        "    def get_action(self, state):\n",
        "        \"\"\"Select action based on spike counts\"\"\"\n",
        "        self.input_group.rates = self.state_encodings[state] * 100*Hz\n",
        "        self.net.run(50*ms)\n",
        "        spike_counts = np.bincount(self.spike_mon.i, minlength=self.n_actions)\n",
        "        return np.argmax(spike_counts)\n",
        "\n",
        "    def update_weights(self, reward):\n",
        "        \"\"\"Apply global reward signal\"\"\"\n",
        "        self.synapses.R = reward"
      ],
      "metadata": {
        "id": "_flSM-LVoSL1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 4: Grid-World Environment\n",
        "\n",
        "## Overview\n",
        "The `GridWorld` class provides a simple **discrete reinforcement learning environment** where:\n",
        "- The agent **navigates a grid** to reach the goal.\n",
        "- **Actions** move the agent **Up (0), Down (1), Left (2), Right (3)**.\n",
        "- **Sparse rewards** are used:  \n",
        "  - **+1.0** if the agent reaches the goal.\n",
        "  - **-0.01** penalty for each step to encourage efficiency.\n",
        "\n",
        "---\n",
        "\n",
        "## Grid Representation\n",
        "Each state is represented by a **single index**, mapped from its $(x,y)$ position:\n",
        "$$\n",
        "\\text{State Index} = x \\times \\text{Grid Size} + y\n",
        "$$"
      ],
      "metadata": {
        "id": "nvehN_1-soHz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 4: GRID-WORLD ENVIRONMENT\n",
        "class GridWorld:\n",
        "    def __init__(self, size=4):\n",
        "        self.size = size\n",
        "        self.goal = (size-1, size-1)\n",
        "        self.state = (0, 0)\n",
        "\n",
        "    def reset(self):\n",
        "        self.state = (0, 0)\n",
        "        return self.obs()\n",
        "\n",
        "    def obs(self):\n",
        "        \"\"\"Convert grid position to state index\"\"\"\n",
        "        return self.state[0] * self.size + self.state[1]\n",
        "\n",
        "    def step(self, action):\n",
        "        x, y = self.state\n",
        "        # Action mapping: 0=Up, 1=Down, 2=Left, 3=Right\n",
        "        if action == 0: y = min(y+1, self.size-1)\n",
        "        elif action == 1: y = max(y-1, 0)\n",
        "        elif action == 2: x = max(x-1, 0)\n",
        "        elif action == 3: x = min(x+1, self.size-1)\n",
        "\n",
        "        self.state = (x, y)\n",
        "        done = (self.state == self.goal)\n",
        "        reward = 1.0 if done else -0.01  # Sparse reward\n",
        "        return self.obs(), reward, done"
      ],
      "metadata": {
        "id": "n7JFqxSgoxoz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 5: Training and Analysis\n",
        "\n",
        "## Overview\n",
        "The **training process** optimizes the agent’s behavior using **reward-modulated STDP**:\n",
        "- The **agent interacts with the environment** for multiple episodes.\n",
        "- **Synaptic weights update** based on cumulative rewards.\n",
        "- **Learning performance** is analyzed through reward trends and synaptic adaptations.\n",
        "\n",
        "---\n",
        "\n",
        "## Training Procedure\n",
        "Each episode follows these steps:\n",
        "1. **Reset the environment** (start at initial state).\n",
        "2. **Agent selects actions** based on spike activity.\n",
        "3. **Environment updates state and provides reward**.\n",
        "4. **Synaptic weights adjust** based on final episode reward.\n",
        "\n",
        "### Reward-Modulated Learning:\n",
        "$$\n",
        "w = \\text{clip} \\left( w + R \\cdot ACh_{\\text{level}} \\cdot (a_{\\text{post}} - a_{\\text{pre}}), 0, w_{\\max} \\right)\n",
        "$$"
      ],
      "metadata": {
        "id": "sNSJmfqes0OB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Step 5: TRAINING AND ANALYSIS\n",
        "def train_agent(agent, env, episodes=100):\n",
        "    \"\"\"Run RL training\"\"\"\n",
        "    rewards = []\n",
        "    steps = []\n",
        "\n",
        "    for _ in tqdm(range(episodes)):\n",
        "        state = env.reset()\n",
        "        total_reward = 0\n",
        "        step = 0\n",
        "        done = False\n",
        "\n",
        "        while not done and step < 100:\n",
        "            action = agent.get_action(state)\n",
        "            next_state, reward, done = env.step(action)\n",
        "\n",
        "            total_reward += reward\n",
        "            state = next_state\n",
        "            step += 1\n",
        "\n",
        "        # Update weights with final reward\n",
        "        agent.update_weights(total_reward)\n",
        "        rewards.append(total_reward)\n",
        "        steps.append(step)\n",
        "\n",
        "    return rewards, steps\n",
        "\n",
        "def analyze_results(results):\n",
        "    \"\"\"Plot comparative results\"\"\"\n",
        "    plt.figure(figsize=(14, 5))\n",
        "\n",
        "    # Learning curves\n",
        "    plt.subplot(1, 2, 1)\n",
        "    for cond, data in results.items():\n",
        "        plt.plot(np.convolve(data['rewards'], np.ones(10)/10, mode='valid'),\n",
        "                label=f\"{cond} (ACh={data['params']['ACh']}, GABA={data['params']['GABA']})\")\n",
        "    plt.xlabel(\"Episode\")\n",
        "    plt.ylabel(\"Smoothed Reward\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Learning Performance\")\n",
        "\n",
        "    # Final weight distributions\n",
        "    plt.subplot(1, 2, 2)\n",
        "    for i, (cond, data) in enumerate(results.items()):\n",
        "        plt.hist(data['final_weights'], bins=20, alpha=0.5,\n",
        "                label=cond, density=True)\n",
        "    plt.xlabel(\"Synaptic Weight\")\n",
        "    plt.ylabel(\"Density\")\n",
        "    plt.legend()\n",
        "    plt.title(\"Weight Distributions\")\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "metadata": {
        "id": "6Z-Ygamyo9RZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Experimental Conditions and Neuromodulation\n",
        "\n",
        "## Overview\n",
        "The experiment evaluates **neuromodulatory influences** on reinforcement learning by modifying:\n",
        "- **Acetylcholine (ACh)** levels (affects synaptic learning rate).\n",
        "- **GABA** levels (regulates inhibitory strength).\n",
        "- Performance is tracked via **reward trends** and **synaptic weight distributions**.\n",
        "\n",
        "---\n",
        "\n",
        "## Defined Conditions\n",
        "The agent is trained under three different **neuromodulation settings**:\n",
        "\n",
        "| **Condition**   | **ACh Level** | **GABA Level** |\n",
        "|---------------|------------|------------|\n",
        "| **Control**   | 1.0        | 1.0        |\n",
        "| **High ACh**  | 2.0        | 1.0        |\n",
        "| **High GABA** | 1.0        | 2.0        |"
      ],
      "metadata": {
        "id": "eOopJ4eDtD2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "conditions = {\n",
        "    \"Control\": {\"ACh\": 1.0, \"GABA\": 1.0},\n",
        "    \"High ACh\": {\"ACh\": 2.0, \"GABA\": 1.0},\n",
        "    \"High GABA\": {\"ACh\": 1.0, \"GABA\": 2.0},\n",
        "}\n",
        "\n",
        "results = {}\n",
        "env = GridWorld()\n",
        "\n",
        "for cond_name, params in conditions.items():\n",
        "    print(f\"\\nTraining {cond_name} condition...\")\n",
        "    agent = RSTDPAgent(n_states=16, n_actions=4,\n",
        "                      ACh_level=params[\"ACh\"],\n",
        "                      GABA_level=params[\"GABA\"])\n",
        "\n",
        "    rewards, steps = train_agent(agent, env, episodes=200)\n",
        "\n",
        "    # Store results\n",
        "    results[cond_name] = {\n",
        "        \"params\": params,\n",
        "        \"rewards\": rewards,\n",
        "        \"steps\": steps,\n",
        "        \"final_weights\": agent.state_mon.w[-1]  # Final weights\n",
        "    }\n",
        "\n",
        "# Analyze and plot\n",
        "analyze_results(results)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-At8XAPfRkUH",
        "outputId": "657b5dec-e197-4760-d4eb-736e4a1f849f"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Training Control condition...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            " 58%|█████▊    | 117/200 [1:52:08<1:17:03, 55.70s/it]"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Regarding Biological Insights of Neuromodulation in Learning...\n",
        "\n",
        "| **Aspect**         | **Acetylcholine (ACh)**                                      | **GABA**                                      |\n",
        "|--------------------|-------------------------------------------------------------|-----------------------------------------------|\n",
        "| **Effects**       | Matches empirical observations of ACh enhancing cortical plasticity in attention tasks (Hasselmo, 2006) | Mimics GABAergic control of network stability |\n",
        "| **Implementation** | Scaling factor on STDP                                      | Prevents epileptic-like runaway excitation    |\n",
        "\n",
        "## Acetylcholine (ACh)\n",
        "- **Role in Cognition**: Empirical studies suggest ACh enhances cortical plasticity, particularly in attention-driven tasks (Hasselmo, 2006).\n",
        "- **Implementation in Model**: Acts as a **scaling factor** on STDP learning, modifying synaptic adjustments based on reinforcement signals.\n",
        "\n",
        "## GABA (Gamma-Aminobutyric Acid)\n",
        "- **Function in Stability**: Mimics the natural role of GABAergic inhibition, regulating network excitability and preventing instability.\n",
        "- **Implementation in Model**: Enhances **inhibitory control**, preventing runaway excitation similar to how GABAergic mechanisms maintain balance in biological neural networks."
      ],
      "metadata": {
        "id": "jY-ztdZrtZd1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Limitations in Model Representation\n",
        "\n",
        "| **Issue**                 | **Description**                                       |\n",
        "|--------------------------|-----------------------------------------------------|\n",
        "| **Reward Propagation**   | Simplistic approach (global vs. targeted)           |\n",
        "| **Receptor Dynamics**    | Abstracted receptor interactions (no NMDA/GABA$_B$) |\n",
        "\n",
        "### Reward Propagation\n",
        "- The model employs **global reward signaling**, which lacks the targeted reinforcement observed in biological reward circuits.\n",
        "- Real-world neuromodulation typically involves **localized and context-specific plasticity**, rather than a uniform scaling factor.\n",
        "\n",
        "### Receptor Dynamics\n",
        "- Abstracts **complex receptor interactions**, notably lacking NMDA-mediated plasticity or GABA$_B$ inhibition mechanisms.\n",
        "- Biological learning relies on **interactions between multiple receptor types**, creating more nuanced modulation than the simplified implementation here."
      ],
      "metadata": {
        "id": "UZEXov9qt4Wa"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Summary\n",
        "This model successfully captures **key neuromodulatory effects**, but remains a **simplified approximation** of the intricate biochemical processes underlying reinforcement learning. Improvements could involve **targeted reward propagation** and **expanded receptor mechanisms** to enhance biological fidelity."
      ],
      "metadata": {
        "id": "_soTlZnrt-UG"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# References\n",
        "\n",
        "1. Hasselmo, M. E. (2006). Acetylcholine and learning. *Physiology & Behavior, 87*(4), 450-458. [https://doi.org/10.1016/j.physbeh.2006.11.018](https://doi.org/10.1016/j.physbeh.2006.11.018)  \n",
        "\n",
        "2. Schultz, W. (2016). Dopamine reward prediction-error signaling: A two-component response. *Trends in Cognitive Sciences, 20*(5), 277-286. [https://doi.org/10.1016/j.tics.2016.03.006](https://doi.org/10.1016/j.tics.2016.03.006)  \n",
        "\n",
        "3. Stimberg, M., Goodman, D. F. M., Benichoux, V., & Brette, R. (2019). Brian 2, an efficient simulator for spiking neural networks. *Frontiers in Neuroinformatics, 13*, 56. [https://doi.org/10.3389/fninf.2019.00056](https://doi.org/10.3389/fninf.2019.00056)\n",
        "\n",
        "4. Gallo, N. (2025). Spike-Timing Dependent Plasticity and Reinforcement Learning [Lecture slides]. AI 689. Brozko et al. 2018.\n"
      ],
      "metadata": {
        "id": "64WWviW0pjst"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VXe_3Tdqr2K8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}